# ricocf/mcp-wolframalpha

- **Description:**
  - An MCP (Model Context Protocol) server and client that enables AI assistants and chat-based applications to access the Wolfram Alpha API for real-time computational knowledge and data. Seamlessly integrates Wolfram Alpha into chat applications, facilitating advanced conversational and computational capabilities.
  - Written in Python and designed to fit directly into the MCP server ecosystem.

- **Source:** [https://github.com/ricocf/mcp-wolframalpha](https://github.com/ricocf/mcp-wolframalpha)

- **Category:** api-integration-mcp-servers

- **Tags:** wolfram-alpha, computation, real-time, ai-integration

## Features

- **Wolfram|Alpha Integration:** Supports computational queries for math, science, and data using the Wolfram Alpha API.
- **Modular Architecture:** Easily extendable to support additional APIs and functionalities.
- **Multi-Client Support:** Can handle interactions from multiple clients or interfaces simultaneously.
- **MCP-Client Example:** Includes an example client using Gemini via LangChain, demonstrating LLM integration for real-time knowledge retrieval.
- **CLI Tool:** Run the client directly from the command line for querying.
- **Docker Support:** Includes Dockerfiles for building and running the client in a containerized environment.
- **Configuration Examples:** Provides configuration templates for integration with VSCode MCP Server and Claude Desktop.
- **Open Source:** Released under the MIT license.

## Installation & Usage
- Clone the repository and set up required environment variables (Wolfram Alpha API key, optional Gemini API key).
- Install dependencies using `pip install -r requirements.txt`.
- Configure for use with VSCode MCP Server or Claude Desktop as needed.
- Run the client via CLI or Docker.

## Pricing
- **Open Source:** Free to use under the MIT license. No paid plans or commercial pricing listed.